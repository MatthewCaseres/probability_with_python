{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Probability Basics\n",
    "\n",
    "## Equally likely events\n",
    "\n",
    "If we flip a fair coin, the probability of getting heads or tails is equal. What this means is that if we flip a coin many times that the percentage of both heads and tails will go to 50%. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "    \n",
    "# Runs n trials of a coin flip and returns the number of heads and tails.\n",
    "def runTrials(n: int) -> Tuple[int, int]:\n",
    "    heads = 0\n",
    "    tails = 0\n",
    "    for i in range(n):\n",
    "      # random.random() returns a random number between 0 and 1. Half the time this value is less than 0.5.\n",
    "        if random.random() < 0.5:\n",
    "            heads += 1\n",
    "        else:\n",
    "            tails += 1\n",
    "    return heads, tails\n",
    "    \n",
    "\n",
    "# Format the output of runTrials.\n",
    "def printResults(heads: int, tails: int) -> None:\n",
    "    print(\"Trials:\", heads + tails, \"Proportion:\", heads/(heads+tails),\"Heads:\", heads, \"Tails:\", tails)\n",
    "\n",
    "# The * operator unpacks a list or tuple into separate variables.\n",
    "printResults(*runTrials(10))\n",
    "printResults(*runTrials(100))\n",
    "printResults(*runTrials(1_000_000))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trials: 10 Proportion: 0.3 Heads: 3 Tails: 7\n",
      "Trials: 100 Proportion: 0.54 Heads: 54 Tails: 46\n",
      "Trials: 1000000 Proportion: 0.50016 Heads: 500160 Tails: 499840\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our previous example we made a **random selection** between heads and tails. A random selection means that each value is equally likely. For example, a randomly selected card from a deck of $52$ cards has probability $1/52$ of being a 2 of clubs.\n",
    "\n",
    "When every possible outcome has equal probability, the formula for the probability of some subset of the outcomes is:\n",
    "\n",
    "$$\\frac{\\text{Number of Selected Outcomes}}{\\text{Number of Total Possible Outcomes}}$$\n",
    "\n",
    "For our coin flipping example, we have $2$ possible outcomes: heads and tails. The number of selected outcomes is $1$ (either heads or tails), so the probability is $1/2$.\n",
    "\n",
    "### Playing cards example\n",
    "\n",
    "**What is the probability of drawing 4 aces when we randomly select 5 cards from a deck?**\n",
    "\n",
    "To draw the 4 aces, we must select 5 cards. 4 of them are aces and 1 of them is not. ${4 \\choose 4}$ is the number of ways to select 4 aces. ${48 \\choose 1}$ is the number of ways to select the non-ace. The number of ways to select the 4 aces is:\n",
    "\n",
    "$${4 \\choose 4}{48 \\choose 1} = 48$$\n",
    "\n",
    "The number of ways to draw 5 cards is:\n",
    "\n",
    "$${52 \\choose 5} = 2598960$$\n",
    "\n",
    "The answer is then:\n",
    "\n",
    "$$\\frac{\\text{Ways to select 4 aces}}{\\text{Ways to select 5 cards}} = \\frac{48}{2598960} = \\frac{1}{54145}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Terminology\n",
    "\n",
    "### Experiments\n",
    "\n",
    "When we flip a coin or roll a die that is an **experiment**. Experiments have a set of possible outcomes that happen with some probability.\n",
    "\n",
    "### Sample space\n",
    "\n",
    "The set of all possible outcomes of an experiment is the **sample space**. When we flip a coin once the sample space is $S=\\{H,T\\}$. When we flip a coin twice the sample space is $S=\\{HH,HT,TH,TT\\}$.\n",
    "\n",
    "### Events\n",
    "\n",
    "**Events** represent a set of possible outcomes from an experiment. When we flip two coins there is an event for flipping two heads, $E_\\text{both heads}=\\{HH\\}$. There is also an event for not flipping two heads $E_\\text{not both heads}=\\{HT,TH,TT\\}$. An event is a subset of the sample space. It may represent a single outcome of our experiment, or it may represent several of the possible outcomes:\n",
    "$$E \\subseteq S$$\n",
    "\n",
    "We can rewrite our formula for probabilities when all outcomes of an experiment are equally likely using $n(E)$ for the number of selected outcomes.\n",
    "$$\\frac{\\text{Number of Selected Outcomes}}{\\text{Number of Total Possible Outcomes}} = \\frac{n(E)}{n(S)}$$\n",
    "\n",
    "### Example\n",
    "\n",
    "**We roll two dice and want to calculate the probability that the dice sum to 4. Define the experiment, sample space, event, and calculate the probability of the event.**\n",
    "\n",
    "The experiment is rolling two dice.\n",
    "\n",
    "The sample space is the set of all possible ordered combinations of two dice. We can define it in set builder notation: \n",
    "$$S=\\{(a,b)|a,b \\in \\{1,2,3,4,5,6\\}\\}$$\n",
    "$n(S) = 36$ because there are $6$ possible outcomes for each die and we use the multiplication principle.\n",
    "\n",
    "The event contains the elements of the sample space that sum to 4. \n",
    "$$E = \\{(1,3),(3,1),(2,2)\\}$$\n",
    "\n",
    "The probability is \n",
    "$$\\frac{\\text{Number of Selected Outcomes}}{\\text{Number of Total Possible Outcomes}} = \\frac{n(E)}{n(S)} = \\frac{3}{36}=\\frac{1}{12}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Probability Functions\n",
    "\n",
    "A probability function $P$ calculates the probability of an event. When we flip a coin twice the probability of getting a head and a tail is:\n",
    "$$P(\\{HT, TH\\})=.5$$\n",
    "\n",
    "For experiments where all outcomes are equally likely:\n",
    "$$P(E) = \\frac{n(E)}{n(S)}$$\n",
    "\n",
    "A probability is between $0$ and $1$ because an event can't happen less than $0\\%$ of the time or more than $100\\%$ of the time. More formally, $P$ is a function that takes an event as input and gives a number between $0$ and $1$ as output. In notation:\n",
    "\n",
    "$$P:E \\mapsto [0,1]$$\n",
    "\n",
    "The rule about probabilities being between $0$ and $1$ is derived from some more basic assumptions and not intuition about the percentage of times something occurs. Let's talk about these basic assumptions.\n",
    "\n",
    "## Probability Axioms\n",
    "There are three fundamental assumptions (called axioms) about probability functions from which our other laws are derived.\n",
    "\n",
    "**First Axiom** - For an event $E$, and probability function $P$:\n",
    "$$P(E) \\geq 0$$\n",
    "\n",
    "**Second Axiom** - For a sample space $S$:\n",
    "$$P(S)=1$$\n",
    "\n",
    "**Third Axiom** - If $E_1, E_2, ...E_n$ are **mutually exclusive** events:\n",
    "$$P(\\bigcup\\limits_{i=1}^{\\infty} E_{i})=P(E_1 \\cup E_2 \\cup...\\cup En \\cup...) = P(E1)+P(E2)+...+P(En)+...$$\n",
    "\n",
    "Recall that mutually exclusive means that sets have empty intersections. We have already seen that $n(E_1 \\cup E_2) = n(E_1) + n(E_2)$ for mutually exclusive events. Our third axiom says something similar for the probability of mutually exclusive events, $P(E_1 \\cup E_2) = P(E_1) + P(E_2)$\n",
    "\n",
    "## Probability Properties\n",
    "\n",
    "From our axioms we can derive important properties of probability functions.\n",
    "\n",
    "For example, We can show that $P(A) + P(A^C) = 1$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A)+P(A^C) &= P(A \\cup A^C) \\text{ by Axiom 3} \\\\\n",
    "&= P(S) = 1 \\text{ by Axiom 2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This implies that $P(A^C) = 1 - P(A)$.\n",
    "\n",
    "Here are some useful formulas that can be derived from these axioms.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{Complements}: \\ P(A^C) = 1 - P(A) \\\\\n",
    "&\\text{Upper  Bound}:P(A) \\leq 1 \\\\\n",
    "&\\text{Probability of 2 Unions}: P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As with counting, the formula for two unions gives us a formula for 3 unions.\n",
    "\n",
    "\n",
    "$$\n",
    "P(A) + P(B) + P(C) - P(A \\cap B)-P(B \\cap C)-P(C \\cap A) + P(A \\cap B \\cap C)\n",
    "$$\n",
    "\n",
    "Notice that these formulas are the same as our formulas for the size of the set if we swap out $P$ for $n$ and $1$ for $n(S)$.\n",
    "\n",
    "***\n",
    "\n",
    "**Example: Coin Flipping and Axioms**\n",
    "\n",
    "If we flip a coin, heads and tails are mutually exclusive events. \n",
    "$$P(\\{H\\} \\cup \\{T\\}) = P(\\{H\\})+P(\\{T\\}) = .5 + .5 = 1$$ \n",
    "We know that $P(\\{H\\}), P(\\{T\\})=.5$ from our formula for equally likely events at the beginning of the chapter. Note that $S = \\{H\\} \\cup \\{T\\}$ so this example also illustrates that $P(S)=1$.\n",
    "\n",
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conditional Probability\n",
    "\n",
    "Suppose we have some cats and dogs that are either fluffy or not fluffy.\n",
    "\n",
    "|  Row title | Fluffy | Not fluffy | Total |\n",
    "| --- | --- | --- | --- |\n",
    "| **Cats** | 21 | 18 | **39** |\n",
    "| **Dogs** | 35 | 26 | **61** |\n",
    "| **Total** | **56** | **44** | **100** |\n",
    "\n",
    "We can calculate the probability of a randomly selected pet being fluffy using our formula for equally likely events.\n",
    "\n",
    "$$\\frac{\\text{Number of Fluffy Pets}}{\\text{Total Number of Pets}} = \\frac{n(\\text{Fluffy})}{n(\\text{Pets})} = \\frac{56}{100}$$\n",
    "\n",
    "We can calculate the probability of a pet being a fluffy cat as:\n",
    "\n",
    "$$\\frac{\\text{Number of Fluffy Cats}}{\\text{Total Number of Pets}} = \\frac{n(\\text{Fluffy} \\cap \\text{Cat})}{n(\\text{Pets})} = \\frac{21}{100}$$\n",
    "\n",
    "We can also calculate the probability of a pet being fluffy, given that the pet is a cat. Notice that the denominator is not all the pets, but only the number of cats.\n",
    "\n",
    "$$\\frac{\\text{Number of Fluffy Cats}}{\\text{Total Number of Cats}} = \\frac{n(\\text{Fluffy} \\cap \\text{Cat})}{n(\\text{Cats})}=\\frac{21}{39}$$\n",
    "\n",
    "This is a **conditional probability** because it is conditioned on the pet being a cat. There is notation for this.\n",
    "\n",
    "$$P(\\text{Fluffy|Cat}) = P(\\text{Pet is fluffy, given that the pet is a cat})$$\n",
    "\n",
    "Notice that $P(\\text{Fluffy|Cat})$ does not mean the same thing as $P(\\text{Fluffy})$.\n",
    "\n",
    "### Formulas\n",
    "\n",
    "The probability of event $A$ given that event $B$ has occurred is denoted $P(A|B)$ and pronounced \"the probability of A given B\". \n",
    "The general formula for this is $$P(A|B)=\\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "We can calculate $P(A \\cap B)$ by rearranging the formula. \n",
    "\n",
    "$$P(A|B) \\cdot P(B) = P(A \\cap B)$$\n",
    "\n",
    "Conceptuaize this as an event B happening with probability $P(B)$ and then an event $A$ happening with probability $P(A|B)$, so $P(A \\cap B) = P(B) \\cdot P(A|B)$\n",
    "\n",
    "For experiments where all outcomes have equal probabilities $P(A|B)$ can be calculated with set sizes. Here is a derivation:\n",
    " $$P(A|B)=\\frac{n(A \\cap B)/n(S)}{n(B)/n(S)}=\\frac{n(A \\cap B)}{n(B)}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Independence\n",
    "\n",
    "Events are **independent** if they do not influence each other. The second flip of a coin is not influenced by the first flip, so the two events are independent.\n",
    "\n",
    "If two events influence each other they are not independent. Let the event that the first card pulled without replacement from a deck is a jack be $J_1$ and the event that the second card pulled is a jack is $J_2$. If I pull a jack on my first try there will be less jacks in the deck for the second draw. The event $J_1$ has an effect on $J_2$ and these events are not independent.\n",
    "\n",
    "***\n",
    "**Definition of Independence**: Events $A$ and $B$ are said to be independent if $P(A|B)=P(A)$.\n",
    "***\n",
    "\n",
    "This definition is equivalent to $P(A \\cap B) = P(A) \\cdot P(B)$ after substitution with the conditional probability formulas. This identity has it's own name:\n",
    "\n",
    "***\n",
    "**Multiplication Rule for Independent Events**: If $A$ and $B$ are independent events then $P(A \\cap B) = P(A) \\cdot P(B)$.\n",
    "***\n",
    "\n",
    "Let $H_n$ be the event that the nth flip of a coin is heads so that $H_2$ means the second coin flip is heads. The multiplication rule for independent events works for more than two events if all events are mutually independent so that $P(H_1 \\cap H_2 \\cap H_3 \\cap H_4)=P(H_1)\\times P(H_2)\\times P(H_3)\\times P(H_4) = (\\frac{1}{2})^4$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "### Bayes Theorem Intuition\n",
    "\n",
    "Suppose 1% of the population uses drugs. 98% of drug users test positive on a drug test and 2% of non-users test positive. What is the probability that a person testing positive for a drug test has used drugs? \n",
    "\n",
    "![](images/03-probability/BayesTreeSmall.PNG)\n",
    "\n",
    "Using the above tree diagram and the definition of conditional probability we calculate:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(\\text{User}|\\text{Positive}) &= \\frac{P(\\text{Positive} \\cap \\text{User})}{P(\\text{Positive})} \\\\\n",
    "&= \\frac{P(\\text{Positive} \\cap \\text{User})}{P(\\text{Positive} \\cap \\text{Not User}) + (\\text{Positive} \\cap \\text{User})} \\\\\n",
    "&= \\frac{P(\\text{Positive}|\\text{User}) \\times P(\\text{User})}{P(\\text{Positive}|\\text{User}) \\times P(\\text{User})+P(\\text{Positive}|\\text{Not User}) \\times P(\\text{Not User})} \\\\\n",
    "&= \\frac{.0099}{.0099+.0099} = \\frac{1}{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's explain these steps in more detail.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Law of Total Probability\n",
    "\n",
    "In the example above we use the following identity:\n",
    "\n",
    "$$P(\\text{Positive}) = P(\\text{Positive} \\cap \\text{Not User}) + (\\text{Positive} \\cap \\text{User})$$\n",
    "\n",
    "This identity works because:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A &= A \\cap S \\text{, } S \\text{ is the sample space} \\\\\n",
    "&= A \\cap (B \\cup B^C) \\text{ because } S = B \\cup B^C \\\\\n",
    "&= (A \\cap B) \\cup (A \\cap B^C) \\text{ via the distributive property} \\\\\n",
    "&\\implies P(A) = P((A \\cap B) \\cup (A \\cap B^C))  \\\\\n",
    "&= P(A \\cap B) + P(A \\cap B^C) \\text{ because disjoint sets} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "There is a more general formulation of this known as the **law of total probability**:\n",
    "\n",
    "***\n",
    "Events $A_1,A_2,...,A_n$ are said to be a **partition** of the sample space $S$ if $A_1 \\cup A_2 \\cup... \\cup A_n=S$ and if for all $i,j: \\ A_i \\cap A_j = \\emptyset$. This just means that the sets $A_1,A_2,...,A_n$ cover the whole set and there is no overlap between the sets.\n",
    "\n",
    "If sets $A_1,A_2,...,A_n$ partition $S$ then for any event $E$ we have **the law of total probability**:\n",
    "$$P(E) = P(E \\cap (A_1 \\cup A_2 \\cup ... \\cup A_n)) = \\\\ P(E \\cap A_1) + P(E \\cap A_2) + ... + P(E \\cap A_n) = \\\\ P(E|A_1) \\cdot P(A_1) +...+ P(E|A_n) \\cdot P(A_n)$$\n",
    "***\n",
    "\n",
    "This visual might be useful:\n",
    "\n",
    "![](images/03-probability/totalprob.PNG)\n",
    "\n",
    "Consider how $P(E) = P(E \\cap (A_1 \\cup A_2 \\cup ... \\cup A_n))$ and then revisit the algebra."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bayes Theorem Formula\n",
    "\n",
    "For a partition $A_1,A_2,...,A_n$ of event $E$:\n",
    "\n",
    "$$P(A_k|E) = \\frac{P(E|A_k) \\cdot P(A_k)}{P(E|A_1) \\cdot P(A_1) +...+ P(E|A_n) \\cdot P(A_n)}$$\n",
    "\n",
    "Returning to our example of drug use, people either are drug users or aren't. So the drug use partitions people that are tested for drugs.\n",
    "\n",
    "$$P(\\text{User}|\\text{Positive}) = \\frac{P(\\text{Positive}|\\text{User}) \\cdot P(\\text{User})}{P(\\text{Positive}|\\text{User}) \\cdot P(\\text{User})+P(\\text{Positive}|\\text{Not User}) \\cdot P(\\text{Not User})}$$\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}